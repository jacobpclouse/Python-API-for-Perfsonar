# Project Overview and History - 525 Wireless Networks

###### About PerfSONAR:
PerfSONAR is a toolkit used to collect real time data about networks, finding bottlenecks and allowing us to get a picture of how they interact with the internet at large. 

Specifically PerfSONAR answers questions such as “How do you gauge performance once data leaves your network?” and “How do we get an accurate measurement of networks around us?”

Tools like this are what enable us to maintain a consistent and smooth network experience for users across the globe!


Spring 2022 - How I went about this project:
When I learned what PerfSONAR was and started to get into the documentation, I knew that I needed to create my own individual server node. This would allow me to actively tinker with various aspects of server setup and force me to learn new technologies like centos, openssh, RESTful APIs, and more. 

The initial setup did not go without incident; issues occurred mainly with the initial installation and the setup with the toolkit itself. The website offered several different distribution methods to use the toolkit: Installing it on a pre-existing ubuntu server; installing a pre-configured iso of centos 7 with the full suite of tools available; or installing a cut down netinstall of centos 7 that would later download the tools over the internet. 

I initially tried to install the toolkit on a PC inside Dr. Zheleva’s Lab on one of the spare workstations. I originally had installed the latest version of Ubuntu Server and then attempted to fetch the toolkit with apt-get, but I kept running into missing dependency errors. When I would download one dependency, another missing dependency error would occur. 

I ended up having to download an entirely different cli interface called ‘aptitude’ in order to install the toolkit as I could not make progress with apt-get. After I installed the toolkit, I attempted to see what tests were running on the server via the web browser. However, I could not access the server via a browser and, upon further inspection, it looked like not all of the tools had been installed. I attempted to install the remaining tools, but was unable to. 

I then decided to try the centos iso image as it seemed to be a more cohesive package. I downloaded the full image iso and flashed it to a flash drive using DD. The install seemed to proceed normally until I was prompted to reboot, at that point the workstation did not recognize any bootable partitions on the hard drive. I went into the bios, tried toggling secure boot, tried enabling virtualization, and tried disabling all the boot options besides the hard drive, but nothing worked. I decided to flash the Netinstall version of the ISO to my thumb drive and flashed that onto the computer. Again, the install seemed to go normally until the first reboot and again the computer could not recognize centos as a bootable os in the bios. I ended up re-installing ubuntu server onto the workstation to confirm that it was not defective hardware, and it recognized it immediately. Clearly, I need to switch up my hardware and methods.

I then decided to use one of my own older laptops at home to run the toolkit since I had two old business laptops from 2007 running core 2 duos. Because the latest versions of Ubuntu server require secure boot and my two laptops only had legacy bios options, my only real option was to use centos. Although the full install iso would not post no matter what I tried, I was able to get the netinstall iso up and running on the PC without an issue. At this time, my first laptop was set up, wired in and ready to go--and that is all I really needed at the moment. 

Because I wanted to set up the second laptop as a test server so I could install software onto it without having to worry about it interfering with any PerfSONAR processes, I bought a newer cpu and power supply for it. I also installed the vanilla version of centos 7 so I would be able to use the same commands and software across both machines. I was able to successfully login, setup openssh, setup x11, and install a GUI in case I needed to access the web for anything and download something else.

I initially configured the tests that I wanted to run via logging into the web gui and getting into the config tab. I had to select nodes to test against (i.e. to bounce measurements to and from) and to take the kind of measurements that I wanted to take. Finding nodes turned out to be quite easy: PerfSONAR catalogs list available nodes in its ‘global node directory’ allowing you to see where they were located, who operated them and what test types they supported. I wanted to get a good collection of samples, at least 5 on each continent. At first, I started out small, only measuring 4 nodes at a time and the PC seemed to handle it well. I then tried ramping up to around 35 nodes with each node having all possible tests run on it. This was a bit too much for my older laptop to handle and it slowed down to a crawl, making it impossible to access the web GUI and turning off all of the tests. I tried finding a way to disable tests within the operating system itself, but I could not find anything within the documentation. In turn, I was forced to backup and wipe the machine. I ended up swapping in the other machine and configuring it the same as the first host with the key difference being that I only was pinging 10 hosts now. This went much better than the original setup and I was able to access the web GUI with no issues while I gathered a sizable amount of data. 

Now that I had the hardware setup and data streams were steadily coming in, I needed to find a way of accessing that data remotely. Since I did not know how to do this, I needed to more clearly define my end goal to determine a way forward. At first, I wanted to do a full pentest against my servers, but it did not fit into the overall scope of the class. Ultimately, I decided to visualize the data I was collecting with a third party software called Grafana. This was software that you installed on your local machine to run a local, customizable dashboard. You could then import data into this dashboard and choose what kind of output graph you needed, all within your web browser. I installed Grafana on my second centos box and tried to configure it so that it would be available to any computer on my local network, but I ran into trouble with setting it up this way. In the end, I was forced to set up a desktop GUI on my centos machine so I could access my Grafana instance. I tried using two different plugins in order to import JSON files into my dashboard, but I couldn’t get them to work. In the end I couldn’t use Grafana for this reason and looked for an alternative. 

I started using another visualization tool called Exploratory, which promised to be a great tool to visualize custom data sets. But before I could proceed with Exploratory, I needed to suss out how I was going to get data from my server since one of the biggest gaps in my knowledge was getting data from RESTful API's. I knew about GET and POST commands from taking the Computer Networks class in Fall 2021, but I had never really written a program to pass parameters to an API, store the data coming back, perform operations on that data, and then visualize the result. Because I didn't know where to start, so I reached out to Karyn for some advice since she had previously worked with APIs for the eapp project. She recommended that I look into Python to interact with the API and use the matlib library to graph different functions, both of which seemed like solid ideas. I was able to find a video on youtube that went over the basics of grabbing data from APIs with Python, formatting the data and storing the output as JSON files. I also used software called Postman to test GET and POST requests to my local API, and I confirmed that it was accepting data correctly. 

Through my research I discovered that when you go to the web interface of your server, it gives you the URI of the RESTful API. (Note that you can also look at the PerfSONAR Esmond API docs to get this information as well.) I then wrote a script based off of the videos I watched and successfully created a JSON with all the data I would need. I used the API docs to select specific parameters (i.e., select the specific 'event types' that were being run against each node) and pulled data related to specific tests. This was important since it allowed me to pull in specific data sets which could be used with exploratory, and I was able to create my first graph using the Azerbaijan throughput test. (See the url to the graph to view it).

After that, I wrote a program that grabbed specific test files for the entire list of hosts, but it initially had an error each time it would grab the data from one host and copy it to all the others. I then wrote a script that would just grab the event data for a single host and then paired it with a bash script to run the first script with all of the hosts I had. I also worked on adapting the script to query remote nodes as well so I could: 1) find all the hosts they are connected to; and 2) then grab the event type data for each of those hosts.

As of this writing, I am working on polishing up my scripts and creating more graph visualizations for my project. 

Conclusion:

The hardest part of this independent study was identifying gaps in my knowledge that I didn't realize existed. When I first started, I had no idea how I was even going to collect the data in the first place, much less download it and graph it. It was very intimidating, but I realized the importance of asking questions early on. I have learned more about APIs, servers, JSON files and Python in these three months than I did in the last five years, to the point where I feel comfortable using them. It was fun being given total freedom to experiment with new technologies and see what works, certainly an experience I have never had before. I was in control of my success, I was in charge of getting myself the resources that I needed, and I am proud of the progress that I have made. 

Dr. Zheleva, thank you for giving me this opportunity and trusting me to contribute to the curriculum of your next class. It made me trust my abilities more!

